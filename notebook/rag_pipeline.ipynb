{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69556320",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f105c295",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dolapo/Downloads/biblestudyapp/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import chromadb\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b75ca53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book 01 Genesis\n",
      "001:001 In the beginning God{After \"God,\" the Hebrew has the two letters\n",
      "        \"Aleph Tav\" (the first and last letters of the Hebrew alphabet)\n",
      "        as a grammatical marker.} created the heavens and the earth.\n",
      "001:002 Now the eart\n"
     ]
    }
   ],
   "source": [
    "loader=TextLoader(\"../data/whole_bible.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document[0].page_content[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10634fe2",
   "metadata": {},
   "source": [
    "### loading text file and adding metadata to docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b6405e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/whole_bible.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543734fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31102 verses loaded as Document objects with metadata.\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "current_book = None\n",
    "current_verse_text = \"\"\n",
    "current_chapter = None\n",
    "current_verse = None\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "\n",
    "        # Detect book line\n",
    "        if line.startswith(\"Book\"):\n",
    "            current_book = line.split(\" \", 2)[2]  # 'Genesis'\n",
    "            continue\n",
    "\n",
    "        # Detect cghapter and verse line\n",
    "        match = re.match(r\"(\\d{3}):(\\d{3})\\s+(.*)\", line)\n",
    "        if match:\n",
    "            # Save previous verse if exists\n",
    "            if current_verse_text:\n",
    "                doc = Document(\n",
    "                    page_content=current_verse_text,\n",
    "                    metadata={\n",
    "                        \"book\": current_book,\n",
    "                        \"chapter\": current_chapter,\n",
    "                        \"verse\": current_verse,\n",
    "                        \"source\": f\"{current_book}_{current_chapter}_{current_verse}\"\n",
    "                    }\n",
    "                )\n",
    "                documents.append(doc)\n",
    "                current_verse_text = \"\"\n",
    "\n",
    "            # New verse\n",
    "            current_chapter = int(match.group(1))\n",
    "            current_verse = int(match.group(2))\n",
    "            current_verse_text = match.group(3)\n",
    "        else:\n",
    "            # Continuation of previous verse (multi-line)\n",
    "            current_verse_text += \" \" + line\n",
    "\n",
    "# Add last verse\n",
    "if current_verse_text:\n",
    "    doc = Document(\n",
    "        page_content=current_verse_text,\n",
    "        metadata={\n",
    "            \"book\": current_book,\n",
    "            \"chapter\": current_chapter,\n",
    "            \"verse\": current_verse,\n",
    "            \"source\": f\"{current_book}_{current_chapter}_{current_verse}\"\n",
    "        }\n",
    "    )\n",
    "    documents.append(doc)\n",
    "print(f\"{len(documents)} verses loaded as Document objects with metadata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e30b6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Hamor and Shechem, his son, came to the gate of their city, and talked with the men of their city, saying,' metadata={'book': 'Genesis', 'chapter': 34, 'verse': 20, 'source': 'Genesis_34_20'}\n",
      "page_content='The sons of Eliezer were:  Rehabiah the chief; and Eliezer had no other sons; but the sons of Rehabiah were very many.' metadata={'book': '1 Chronicles', 'chapter': 23, 'verse': 17, 'source': '1 Chronicles_23_17'}\n",
      "page_content='Son of man, tell her, You are a land that is not cleansed, nor rained on in the day of indignation.' metadata={'book': 'Ezekiel', 'chapter': 22, 'verse': 24, 'source': 'Ezekiel_22_24'}\n",
      "page_content='However much she glorified herself, and grew wanton, so much give her of torment and mourning.  For she says in her heart, 'I sit a queen, and am no widow, and will in no way see mourning.'' metadata={'book': 'Revelation', 'chapter': 18, 'verse': 7, 'source': 'Revelation_18_7'}\n"
     ]
    }
   ],
   "source": [
    "for doc in documents[1000::10000]:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9e0d9",
   "metadata": {},
   "source": [
    "## Embedding and Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a84cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts...\")\n",
    "        embeddings = self.model.encode(texts)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12efaa97",
   "metadata": {},
   "source": [
    "## Handling duplicated bible verses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c2078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 16 duplicates\n"
     ]
    }
   ],
   "source": [
    "def deduplicate_documents(documents: List[Document]) -> List[Document]:\n",
    "    seen = {}\n",
    "    unique_docs = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        verse_id = doc.metadata['source']\n",
    "        if verse_id not in seen:\n",
    "            seen[verse_id] = True\n",
    "            unique_docs.append(doc)\n",
    "    \n",
    "    print(f\"Removed {len(documents) - len(unique_docs)} duplicates\")\n",
    "    return unique_docs\n",
    "\n",
    "documents = deduplicate_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec1cc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-mpnet-base-v2\n",
      "Model loaded successfully. Embedding dimension: 768\n",
      "Generating embeddings for 31086 texts...\n",
      "Generated embeddings with shape: (31086, 768)\n"
     ]
    }
   ],
   "source": [
    "embedding_manager=EmbeddingManager()\n",
    "texts = [doc.page_content for doc in documents]\n",
    "embeddings = embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39516767",
   "metadata": {},
   "source": [
    "### Create VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fae414de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages Bible verse embeddings in a ChromaDB vector store\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"bible_web_nt\",\n",
    "        persist_directory: str = \"../data/vector_store\"\n",
    "    ):\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "    \n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        os.makedirs(self.persist_directory, exist_ok=True)\n",
    "        self.client = chromadb.PersistentClient(\n",
    "            path=self.persist_directory\n",
    "        )\n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=self.collection_name,\n",
    "            metadata={\n",
    "                \"description\": \"World English Bible verses for Bible study chatbot\",\n",
    "                \"translation\": \"WEB\",\n",
    "                \"granularity\": \"verse\"\n",
    "            }\n",
    "        )\n",
    "        print(f\"Vector store ready: {self.collection_name}\")\n",
    "        print(f\"Existing verses: {self.collection.count()}\")\n",
    "    \n",
    "    def _make_id(self, doc: Document) -> str:\n",
    "        \"\"\"\n",
    "        Create a deterministic ID so verses don't duplicate on re-runs\n",
    "        Example: Genesis_1_1\n",
    "        \"\"\"\n",
    "        meta = doc.metadata\n",
    "        return f\"{meta['book']}_{meta['chapter']}_{meta['verse']}\"\n",
    "    \n",
    "    def add_documents(\n",
    "        self,\n",
    "        documents: List[Document],\n",
    "        embeddings: np.ndarray,\n",
    "        batch_size: int = 5000\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Add documents to the vector store in batches\n",
    "        \n",
    "        Args:\n",
    "            documents: List of Document objects\n",
    "            embeddings: Numpy array of embeddings\n",
    "            batch_size: Number of documents to process at once (default: 5000)\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Documents and embeddings count must match\")\n",
    "        \n",
    "        total_docs = len(documents)\n",
    "        print(f\"Processing {total_docs} documents in batches of {batch_size}...\")\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, total_docs, batch_size):\n",
    "            batch_end = min(i + batch_size, total_docs)\n",
    "            batch_docs = documents[i:batch_end]\n",
    "            batch_embeddings = embeddings[i:batch_end]\n",
    "            \n",
    "            print(f\"\\nBatch {i//batch_size + 1}: Processing verses {i+1} to {batch_end}\")\n",
    "            \n",
    "            ids = []\n",
    "            metadatas = []\n",
    "            texts = []\n",
    "            vectors = []\n",
    "            \n",
    "            for doc, embedding in zip(batch_docs, batch_embeddings):\n",
    "                verse_id = self._make_id(doc)\n",
    "                ids.append(verse_id)\n",
    "                texts.append(doc.page_content)\n",
    "                vectors.append(embedding.tolist())\n",
    "                metadatas.append({\n",
    "                    \"book\": doc.metadata.get(\"book\"),\n",
    "                    \"chapter\": doc.metadata.get(\"chapter\"),\n",
    "                    \"verse\": doc.metadata.get(\"verse\"),\n",
    "                    \"source\": doc.metadata.get(\"source\", verse_id),\n",
    "                    \"translation\": \"WEB\",\n",
    "                    \"content_length\": len(doc.page_content)\n",
    "                })\n",
    "            \n",
    "            try:\n",
    "                self.collection.upsert(\n",
    "                    ids=ids,\n",
    "                    documents=texts,\n",
    "                    embeddings=vectors,\n",
    "                    metadatas=metadatas\n",
    "                )\n",
    "                print(f\"Added {len(ids)} verses to store\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error adding batch {i//batch_size + 1}\")\n",
    "                raise e\n",
    "        \n",
    "        final_count = self.collection.count()\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Complete! Total verses in store: {final_count}\")\n",
    "        print(f\"{'='*50}\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query_embedding: np.ndarray,\n",
    "        n_results: int = 5,\n",
    "        filter_dict: dict = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Search for similar verses\n",
    "        \n",
    "        Args:\n",
    "            query_embedding: Embedding vector for the query\n",
    "            n_results: Number of results to return\n",
    "            filter_dict: Optional metadata filter (e.g., {\"book\": \"Genesis\"})\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with ids, documents, metadatas, and distances\n",
    "        \"\"\"\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=[query_embedding.tolist()],\n",
    "            n_results=n_results,\n",
    "            where=filter_dict\n",
    "        )\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8835f6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store ready: bible_web_nt\n",
      "Existing verses: 0\n",
      "Processing 31086 documents in batches of 5000...\n",
      "\n",
      "Batch 1: Processing verses 1 to 5000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 2: Processing verses 5001 to 10000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 3: Processing verses 10001 to 15000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 4: Processing verses 15001 to 20000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 5: Processing verses 20001 to 25000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 6: Processing verses 25001 to 30000\n",
      "Added 5000 verses to store\n",
      "\n",
      "Batch 7: Processing verses 30001 to 31086\n",
      "Added 1086 verses to store\n",
      "\n",
      "==================================================\n",
      "Complete! Total verses in store: 31086\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "vectorstore=VectorStore()\n",
    "vectorstore.add_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4574f17b",
   "metadata": {},
   "source": [
    "# Retriever pipeline from Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49e9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store, embedding_manager):\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Semantic search for topical queries\"\"\"\n",
    "        print(f\"ðŸ“– Semantic search for: '{query}'\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    similarity_score = 1 - distance\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"âœ“ Found {len(retrieved_docs)} relevant verses\")\n",
    "            else:\n",
    "                print(\"âœ— No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error during retrieval: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def retrieve_with_filter(\n",
    "        self,\n",
    "        query: str,\n",
    "        book: str = None,\n",
    "        chapter: int = None,\n",
    "        top_k: int = 5\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Semantic search with metadata filtering (e.g., search 'love' only in John)\n",
    "        \"\"\"\n",
    "        print(f\"ðŸ“– Semantic search for '{query}' in {book}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        try:\n",
    "            # Build where filter\n",
    "            where_conditions = []\n",
    "            if book:\n",
    "                where_conditions.append({\"book\": {\"$eq\": book}})\n",
    "            if chapter:\n",
    "                where_conditions.append({\"chapter\": {\"$eq\": chapter}})\n",
    "            \n",
    "            where_filter = None\n",
    "            if len(where_conditions) == 1:\n",
    "                where_filter = where_conditions[0]\n",
    "            elif len(where_conditions) > 1:\n",
    "                where_filter = {\"$and\": where_conditions}\n",
    "            \n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k,\n",
    "                where=where_filter\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(\n",
    "                    zip(ids, documents, metadatas, distances)\n",
    "                ):\n",
    "                    similarity_score = 1 - distance\n",
    "                    retrieved_docs.append({\n",
    "                        'id': doc_id,\n",
    "                        'content': document,\n",
    "                        'metadata': metadata,\n",
    "                        'similarity_score': similarity_score,\n",
    "                        'distance': distance,\n",
    "                        'rank': i + 1\n",
    "                    })\n",
    "                \n",
    "                print(f\"âœ“ Found {len(retrieved_docs)} relevant verses\")\n",
    "            else:\n",
    "                print(\"âœ— No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Error during filtered retrieval: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def retrieve_by_reference(\n",
    "        self, \n",
    "        book: str, \n",
    "        chapter: int = None, \n",
    "        verse: int = None,\n",
    "        top_k: int = 50\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve verses by biblical reference (Issue 2 solution)\n",
    "        \n",
    "        Args:\n",
    "            book: Book name (e.g., \"Genesis\")\n",
    "            chapter: Chapter number (optional)\n",
    "            verse: Verse number (optional)\n",
    "            top_k: Maximum results to return\n",
    "        \"\"\"\n",
    "        ref_str = f\"{book}\"\n",
    "        if chapter: ref_str += f\" {chapter}\"\n",
    "        if verse: ref_str += f\":{verse}\"\n",
    "        print(f\"Looking up reference: {ref_str}\")\n",
    "        \n",
    "        try:\n",
    "            # Build ChromaDB where filter with proper $and operator\n",
    "            where_conditions = [{\"book\": {\"$eq\": book}}]\n",
    "            \n",
    "            if chapter is not None:\n",
    "                where_conditions.append({\"chapter\": {\"$eq\": chapter}})\n",
    "            if verse is not None:\n",
    "                where_conditions.append({\"verse\": {\"$eq\": verse}})\n",
    "            \n",
    "            # Use $and if multiple conditions\n",
    "            if len(where_conditions) == 1:\n",
    "                where_filter = where_conditions[0]\n",
    "            else:\n",
    "                where_filter = {\"$and\": where_conditions}\n",
    "            \n",
    "            results = self.vector_store.collection.get(\n",
    "                where=where_filter,\n",
    "                limit=top_k\n",
    "            )\n",
    "            \n",
    "            retrieved_docs = []\n",
    "            if results['documents']:\n",
    "                for i, (doc_id, document, metadata) in enumerate(\n",
    "                    zip(results['ids'], results['documents'], results['metadatas'])\n",
    "                ):\n",
    "                    retrieved_docs.append({\n",
    "                        'id': doc_id,\n",
    "                        'content': document,\n",
    "                        'metadata': metadata,\n",
    "                        'rank': i + 1\n",
    "                    })\n",
    "                \n",
    "                print(f\"Found {len(retrieved_docs)} verse(s)\")\n",
    "            else:\n",
    "                print(\"No verses found for that reference\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during reference retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "class SmartBibleRetriever:\n",
    "    \"\"\"\n",
    "    Smart retrieval combining reference parsing with semantic search (Issue 3 solution)\n",
    "    This is your main interface!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, rag_retriever: RAGRetriever):\n",
    "        self.rag_retriever = rag_retriever\n",
    "        \n",
    "        # Common Bible book variations\n",
    "        self.book_mappings = {\n",
    "            'genesis': 'Genesis', 'gen': 'Genesis',\n",
    "            'exodus': 'Exodus', 'ex': 'Exodus', 'exod': 'Exodus',\n",
    "            'matthew': 'Matthew', 'matt': 'Matthew', 'mt': 'Matthew',\n",
    "            'john': 'John', 'jn': 'John',\n",
    "            '1 john': '1 John', '2 john': '2 John', '3 john': '3 John',\n",
    "            'revelation': 'Revelation', 'rev': 'Revelation',\n",
    "            'psalms': 'Psalms', 'psalm': 'Psalms', 'ps': 'Psalms',\n",
    "            'romans': 'Romans', 'rom': 'Romans',\n",
    "            'acts': 'Acts',\n",
    "            # Add more as needed\n",
    "        }\n",
    "    \n",
    "    def retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Smart retrieval that automatically detects query type\n",
    "        \n",
    "        Args:\n",
    "            query: User's search query\n",
    "            top_k: Number of results to return\n",
    "        \n",
    "        Returns:\n",
    "            List of relevant verses\n",
    "        \"\"\"\n",
    "        # Check for book-specific topical query (e.g., \"love in John\")\n",
    "        book_filter = self._extract_book_filter(query)\n",
    "        \n",
    "        # Try to parse as biblical reference first\n",
    "        ref = self._parse_reference(query)\n",
    "        \n",
    "        if ref:\n",
    "            # Use metadata filtering for specific references\n",
    "            return self.rag_retriever.retrieve_by_reference(\n",
    "                book=ref['book'],\n",
    "                chapter=ref.get('chapter'),\n",
    "                verse=ref.get('verse'),\n",
    "                top_k=top_k\n",
    "            )\n",
    "        elif book_filter:\n",
    "            # Topical search within a specific book\n",
    "            return self.rag_retriever.retrieve_with_filter(\n",
    "                query=query,\n",
    "                book=book_filter,\n",
    "                top_k=top_k\n",
    "            )\n",
    "        else:\n",
    "            # Use semantic search for general topical queries\n",
    "            return self.rag_retriever.retrieve(query, top_k=top_k)\n",
    "    \n",
    "    def _extract_book_filter(self, query: str) -> str:\n",
    "        \"\"\"\n",
    "        Extract book name from queries like 'love in John' or 'faith from Romans'\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Patterns like \"from the book of john\", \"in john\", \"from john\"\n",
    "        patterns = [\n",
    "            r'from\\s+(?:the\\s+book\\s+of\\s+)?(\\d?\\s?[a-z]+)',\n",
    "            r'in\\s+(?:the\\s+book\\s+of\\s+)?(\\d?\\s?[a-z]+)',\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, query_lower)\n",
    "            if match:\n",
    "                book_raw = match.group(1).strip()\n",
    "                book = self._normalize_book_name(book_raw)\n",
    "                if book:\n",
    "                    return book\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _parse_reference(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Parse biblical references from natural language\n",
    "        \n",
    "        Handles patterns like:\n",
    "        - \"Genesis 1:1\"\n",
    "        - \"John 3:16\"\n",
    "        - \"1 John 2:3\"\n",
    "        - \"Genesis chapter 1 verse 1\"\n",
    "        - \"What does Genesis 1:1 say?\"\n",
    "        \n",
    "        Returns:\n",
    "            {'book': str, 'chapter': int, 'verse': int} or None\n",
    "        \"\"\"\n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Pattern 1: \"genesis 1:1\" or \"1 john 3:16\"\n",
    "        pattern1 = r'(\\d?\\s?[a-z]+)\\s+(\\d+)[:\\s]+(\\d+)'\n",
    "        match = re.search(pattern1, query_lower)\n",
    "        \n",
    "        if match:\n",
    "            book_raw = match.group(1).strip()\n",
    "            book = self._normalize_book_name(book_raw)\n",
    "            if book:\n",
    "                return {\n",
    "                    'book': book,\n",
    "                    'chapter': int(match.group(2)),\n",
    "                    'verse': int(match.group(3))\n",
    "                }\n",
    "        \n",
    "        # Pattern 2: \"genesis chapter 1 verse 1\"\n",
    "        pattern2 = r'(\\d?\\s?[a-z]+)\\s+chapter\\s+(\\d+)\\s+verse\\s+(\\d+)'\n",
    "        match2 = re.search(pattern2, query_lower)\n",
    "        \n",
    "        if match2:\n",
    "            book_raw = match2.group(1).strip()\n",
    "            book = self._normalize_book_name(book_raw)\n",
    "            if book:\n",
    "                return {\n",
    "                    'book': book,\n",
    "                    'chapter': int(match2.group(2)),\n",
    "                    'verse': int(match2.group(3))\n",
    "                }\n",
    "        \n",
    "        # Pattern 3: Just book and chapter \"genesis 1\" or \"john 3\"\n",
    "        pattern3 = r'(\\d?\\s?[a-z]+)\\s+(\\d+)(?:\\D|$)'\n",
    "        match3 = re.search(pattern3, query_lower)\n",
    "        \n",
    "        if match3:\n",
    "            book_raw = match3.group(1).strip()\n",
    "            book = self._normalize_book_name(book_raw)\n",
    "            if book:\n",
    "                return {\n",
    "                    'book': book,\n",
    "                    'chapter': int(match3.group(2))\n",
    "                }\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _normalize_book_name(self, book_raw: str) -> str:\n",
    "        \"\"\"Convert various book name formats to standard form\"\"\"\n",
    "        book_lower = book_raw.lower().strip()\n",
    "        \n",
    "        # Check mappings\n",
    "        if book_lower in self.book_mappings:\n",
    "            return self.book_mappings[book_lower]\n",
    "        \n",
    "        # Try title case as fallback\n",
    "        return book_raw.strip().title()\n",
    "    \n",
    "    def format_results(self, results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Pretty print results for display\"\"\"\n",
    "        if not results:\n",
    "            return \"No verses found.\"\n",
    "        \n",
    "        output = []\n",
    "        for result in results:\n",
    "            meta = result['metadata']\n",
    "            reference = f\"{meta['book']} {meta['chapter']}:{meta['verse']}\"\n",
    "            content = result['content']\n",
    "            \n",
    "            if 'similarity_score' in result:\n",
    "                score = result['similarity_score']\n",
    "                output.append(f\"{reference} (similarity: {score:.2f})\\n{content}\\n\")\n",
    "            else:\n",
    "                output.append(f\"{reference}\\n{content}\\n\")\n",
    "        \n",
    "        return \"\\n\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6ccee38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Looking up reference: Genesis 1:1\n",
      "Found 1 verse(s)\n",
      "Genesis 1:1\n",
      "In the beginning God{After \"God,\" the Hebrew has the two letters \"Aleph Tav\" (the first and last letters of the Hebrew alphabet) as a grammatical marker.} created the heavens and the earth.\n",
      "\n",
      "============================================================\n",
      "Looking up reference: John 3:16\n",
      "Found 1 verse(s)\n",
      "John 3:16\n",
      "For God so loved the world, that he gave his one and only Son, that whoever believes in him should not perish, but have eternal life.\n",
      "\n",
      "============================================================\n",
      "ðŸ“– Semantic search for: 'verses about faith'\n",
      "Generating embeddings for 1 texts...\n",
      "Generated embeddings with shape: (1, 768)\n",
      "âœ“ Found 5 relevant verses\n",
      "James 2:22 (similarity: 0.37)\n",
      "You see that faith worked with his works, and by works faith was perfected;\n",
      "\n",
      "Romans 10:17 (similarity: 0.29)\n",
      "So faith comes by hearing, and hearing by the word of God.\n",
      "\n",
      "Romans 1:17 (similarity: 0.28)\n",
      "For in it is revealed God's righteousness from faith to faith. As it is written, \"But the righteous shall live by faith.\"{Habakkuk 2:4}\n",
      "\n",
      "Luke 17:5 (similarity: 0.27)\n",
      "The apostles said to the Lord, \"Increase our faith.\"\n",
      "\n",
      "2 Timothy 2:13 (similarity: 0.26)\n",
      "If we are faithless, he remains faithful.  He can't deny himself.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rag_retriever = RAGRetriever(vectorstore, embedding_manager)\n",
    "smart_retriever = SmartBibleRetriever(rag_retriever)\n",
    "\n",
    "# Example queries\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Reference-based queries (uses metadata filtering)\n",
    "results = smart_retriever.retrieve(\"What does Genesis 1:1 say?\")\n",
    "print(smart_retriever.format_results(results))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = smart_retriever.retrieve(\"John 3 16\")\n",
    "print(smart_retriever.format_results(results))\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Topical queries (uses semantic search)\n",
    "results = smart_retriever.retrieve(\"verses about faith\")\n",
    "print(smart_retriever.format_results(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39a95f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biblestudyapp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
